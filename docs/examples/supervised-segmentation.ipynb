{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from cassetta.losses import LoadableMSE\n",
    "from cassetta.optimizers import LoadableAdam\n",
    "from cassetta.models.segmentation import SegNet\n",
    "from cassetta.datasets import DummySupervisedDataset\n",
    "from cassetta.training.trainers import SimpleSupervisedTrainer, TrainerConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Common variables\n",
    "We will begin by defining some standard and commonly used variables:\n",
    "1. `device`: This is the device that will be used to preform the data generation and training operations.\n",
    "2. `experiment_dir`: The directory in which you want your trained model, associated checkpoints, and experiment parameters to live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "experiment_dir = \"../../models/version_1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Dataset\n",
    "Preparing the dataset for supervised training means establishing a dataset that outputs two tensors: the input tensor (x) and the ground truth tensor (y).\n",
    "\n",
    "Preparing the dataset for supervised training involves several critical steps to ensure that the data is in the right format and quality for the model to learn effectively. Once these criteria are fulfilled, you should have a dataset that outputs two tensors:\n",
    "- **Input Tensor (X)**: Represents the features or input data fed into the model\n",
    "- **Ground Truth Tensor (y)**: Represents the target labels or true values that the model aims to predict.\n",
    "\n",
    "For expediency, we will use a dataset that generates random data for both `X` and `y` called `DummySupervisedDataset`. Note that we've set `x_shape` = `y_shape`. This is because we want to use a UNet for segmentation, and so we'd like our ground truth (and thus, predictions) to be the same size as our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DummySupervisedDataset(\n",
    "    x_shape=(1, 32, 32, 32),\n",
    "    y_shape=(1, 32, 32, 32),\n",
    "    n_classes=None,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Model-Specific Inits\n",
    "We will next initialize the model, the loss function, and the optimzer. Converting the model parameters into a list is quite odd, but we do it so that we can save the model later (not doing so would result in a non-pickleable error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init segmentation network and send to device\n",
    "model = SegNet(3, 1, 1).to(device)\n",
    "# Building loss function (more in `cassetta/losses`) \n",
    "loss = LoadableMSE()\n",
    "# Building optimizer (more in `cassetta/optimizers`)\n",
    "optimizer = LoadableAdam(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Trainer-Specific Inits\n",
    "Now, we will build the trainer configuration object (data container) for storing all trainer-related arguments for customizing the training process. We will also initialize the trainer itself using the loss and the dataset that we've already defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up all training related configurations\n",
    "trainer_config = TrainerConfig(\n",
    "    experiment_dir=experiment_dir,\n",
    "    lr=1e-3,\n",
    "    batch_size=16,\n",
    "    nb_epochs=2,\n",
    "    refresh_experiment_dir=True\n",
    ")\n",
    "\n",
    "# Build trainer\n",
    "trainer = SimpleSupervisedTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    loss=loss,\n",
    "    dataset=dataset,\n",
    "    trainer_config=trainer_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Training\n",
    "We have successfully set everything up! Now, we run the training loop by simply calling the `train()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All contents of ../../models/version_1 have been deleted.\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Saving\n",
    "We can save our trainer (and all of its attributes) with cassetta's custom serialization methods! This makes loading the model much easier at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(f'{experiment_dir}/model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
